{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37d434f0",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8de5458b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from bs4 import BeautifulSoup\n",
    "import string\n",
    "\n",
    "# nltk.download('stopwords')\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('wordnet')\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c6ab280f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Preferences\n",
    "\n",
    "normalize = 'lemmatization' # 'stemming' or 'lemmatization'\n",
    "model = 'linear_svc' # 'svc' or 'linear_svc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "87666b3a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                              review sentiment\n",
      "0  when modi promised “minimum government maximum...  negative\n",
      "1  talk all the nonsense and continue all the dra...   neutral\n",
      "2  what did just say vote for modi  welcome bjp t...  positive\n",
      "3  asking his supporters prefix chowkidar their n...  positive\n",
      "4  answer who among these the most powerful world...  positive\n",
      "sentiment\n",
      "positive    72250\n",
      "neutral     55213\n",
      "negative    35510\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# dataset = pd.read_csv('Twitter_Data.csv')\n",
    "dataset = pd.read_csv('Twitter_Data.csv')\n",
    "\n",
    "# imdb = imdb.iloc[:2000,:]\n",
    "print(dataset.head())\n",
    "print(dataset['sentiment'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f43d06f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total length: 162980\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "72250"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(\"Total length: \" + str(len(dataset)))\n",
    "len(dataset[dataset['sentiment'] == 'positive' ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26e77fa3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Series([], Name: review, dtype: object)\n",
      "Series([], Name: sentiment, dtype: object)\n",
      "X length: 162969\n",
      "Y length: 162969\n"
     ]
    }
   ],
   "source": [
    "# ## Preprocessing\n",
    "dataset.dropna(subset=['review', 'sentiment'], how='any', inplace=True)\n",
    "X = dataset['review']\n",
    "y = dataset['sentiment']\n",
    "\n",
    "# remove neutral reviews\n",
    "# X = X[y != 'neutral']\n",
    "# y = y[y != 'neutral']\n",
    "\n",
    "# print nan\n",
    "print(X[X.isnull()])\n",
    "print(y[y.isnull()])\n",
    "\n",
    "# count of x and y\n",
    "print(\"X length: \" + str(len(X)))\n",
    "print(\"Y length: \" + str(len(y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "56b1c9b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qy/x37f5yq90xg5_8j8gyk95jfh0000gn/T/ipykernel_19818/304066278.py:3: MarkupResemblesLocatorWarning: The input looks more like a filename than markup. You may want to open this file and pass the filehandle into Beautiful Soup.\n",
      "  soup = BeautifulSoup(text, \"html.parser\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    when modi promised “minimum government maximum...\n",
      "1    talk all the nonsense and continue all the dra...\n",
      "2    what did just say vote for modi  welcome bjp t...\n",
      "3    asking his supporters prefix chowkidar their n...\n",
      "4    answer who among these the most powerful world...\n",
      "Name: review, dtype: object\n",
      "162975    why these 456 crores paid neerav modi not reco...\n",
      "162976    dear rss terrorist payal gawar what about modi...\n",
      "162977    did you cover her interaction forum where she ...\n",
      "162978    there big project came into india modi dream p...\n",
      "162979    have you ever listen about like gurukul where ...\n",
      "Name: review, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Remove html tags using beautiful soup\n",
    "def remove_html_tags(text):\n",
    "    soup = BeautifulSoup(text, \"html.parser\")\n",
    "    return soup.get_text()\n",
    "X = X.apply(remove_html_tags)\n",
    "\n",
    "def remove_punctuations(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation, '')\n",
    "    return text\n",
    "X = X.apply(remove_punctuations)\n",
    "\n",
    "\n",
    "print(X.head()) \n",
    "print(X.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "654b1f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize\n",
    "X_tokens = X.apply(word_tokenize)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "354be675",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [when, modi, promised, “, minimum, government,...\n",
      "1    [talk, all, the, nonsense, and, continue, all,...\n",
      "2    [what, did, just, say, vote, for, modi, welcom...\n",
      "3    [asking, his, supporters, prefix, chowkidar, t...\n",
      "4    [answer, who, among, these, the, most, powerfu...\n",
      "Name: review, dtype: object\n"
     ]
    }
   ],
   "source": [
    "print(X_tokens.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fc13a7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mustn', \"it's\", \"isn't\", 'am', 'up', 'too', 'he', 'after', 'himself', 'what', 'o', \"hadn't\", 'weren', 'themselves', 'until', 'those', 'so', 'while', 'both', 'but', 'such', 'her', 'and', 't', \"mustn't\", 'his', 'these', 'has', 'all', 'this', 'nor', 'above', 've', 'here', \"you've\", \"you're\", 'them', 'now', 'how', \"needn't\", 'isn', 'we', 'yourselves', 'were', 'did', 'wouldn', 'into', 'do', \"that'll\", 'd', 'have', 'before', 'when', 'no', 'm', 'at', \"mightn't\", \"aren't\", 'a', 'being', \"doesn't\", 'in', 'been', 'ma', 'hadn', 'which', 'very', 'with', 'is', 'be', 'there', 'your', \"you'll\", 'don', 'they', 'for', 'if', 'shan', 'was', 'to', 'or', 'from', 'ain', 'couldn', 'their', 's', 'y', 'yourself', 'where', 'whom', 'over', 'will', 'few', 'hasn', 'll', 're', 'any', 'haven', \"shan't\", 'own', 'can', \"shouldn't\", \"she's\", 'that', 'who', 'didn', 'its', 'by', 'during', 'as', 'ours', 'does', 'doing', 'it', 'shouldn', 'she', 'itself', 'about', 'of', 'further', 'out', 'once', 'off', 'mightn', 'why', 'doesn', \"haven't\", 'an', 'some', 'me', 'won', \"wouldn't\", 'should', 'myself', \"didn't\", 'because', 'on', \"couldn't\", \"you'd\", 'only', 'had', 'yours', 'then', 'through', 'most', 'under', 'our', 'hers', 'just', \"hasn't\", 'theirs', 'again', \"wasn't\", 'other', 'i', 'herself', 'down', \"weren't\", 'than', \"won't\", 'ourselves', 'the', 'not', 'between', 'aren', 'same', 'wasn', 'having', 'needn', 'below', 'are', 'him', \"don't\", 'each', 'more', \"should've\", 'you', 'against', 'my'}\n",
      "0    [modi, promised, “, minimum, government, maxim...\n",
      "1        [talk, nonsense, continue, drama, vote, modi]\n",
      "2    [say, vote, modi, welcome, bjp, told, rahul, m...\n",
      "3    [asking, supporters, prefix, chowkidar, names,...\n",
      "4    [answer, among, powerful, world, leader, today...\n",
      "Name: review, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(stop_words)\n",
    "\n",
    "\n",
    "X_tokens = X_tokens.apply(lambda x: [item for item in x if item not in stop_words])\n",
    "print(X_tokens.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d038a4cb",
   "metadata": {},
   "source": [
    "Stemming identifies the common root form of a word by removing or replacing word suffixes (e.g. “flooding” is stemmed as “flood”),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e775a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #Stemming\n",
    "from nltk.stem import PorterStemmer\n",
    "if normalize == 'stemming':\n",
    "    ps = PorterStemmer()\n",
    "    X_tokens = X_tokens.apply(lambda x: [ps.stem(item) for item in x])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "53d39878",
   "metadata": {},
   "source": [
    "Lemmatization is a text pre-processing technique used in natural language processing (NLP) models to break a word down to its root meaning to identify similarities. For example, a lemmatization algorithm would reduce the word better to its root word, or lemme, good.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "28f0a8fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    [modi, promised, “, minimum, government, maxim...\n",
      "1        [talk, nonsense, continue, drama, vote, modi]\n",
      "2    [say, vote, modi, welcome, bjp, told, rahul, m...\n",
      "3    [asking, supporter, prefix, chowkidar, name, m...\n",
      "4    [answer, among, powerful, world, leader, today...\n",
      "Name: review, dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Lemmatization\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "if normalize == 'lemmatization':\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    X_tokens = X_tokens.apply(lambda x: [lemmatizer.lemmatize(item) for item in x])\n",
    "\n",
    "print(X_tokens.head())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c53775fb",
   "metadata": {},
   "source": [
    "We will split the dataset into 4:1 ratio of training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ff1235fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_tokens, y, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9eb64547",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorization\n",
    "X_train_seq = [str(x) for x in X_train]\n",
    "X_test_seq = [str(x) for x in X_test]\n",
    "\n",
    "cv = CountVectorizer()\n",
    "X_train_cv = cv.fit_transform(X_train_seq)\n",
    "X_test_cv = cv.transform(X_test_seq)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "468b9548",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(130375, 88384)\n",
      "(32594, 88384)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "88384"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(X_train_cv.shape)\n",
    "print(X_test_cv.shape)\n",
    "\n",
    "len(cv.get_feature_names_out())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8f5f3915",
   "metadata": {},
   "source": [
    "## USING SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "040e7d67",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/sklearn/svm/_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Let's now use SVM to train our model\n",
    "from sklearn.svm import SVC, LinearSVC\n",
    "\n",
    "if model == 'svc':\n",
    "    SVM = SVC()\n",
    "    SVM.fit(X_train_cv, Y_train)\n",
    "\n",
    "elif model == 'linear_svc':\n",
    "    SVM = LinearSVC()\n",
    "    SVM.fit(X_train_cv, Y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "61fd6afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "    negative       0.84      0.81      0.82      7152\n",
      "     neutral       0.89      0.94      0.91     11067\n",
      "    positive       0.91      0.89      0.90     14375\n",
      "\n",
      "    accuracy                           0.89     32594\n",
      "   macro avg       0.88      0.88      0.88     32594\n",
      "weighted avg       0.89      0.89      0.89     32594\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "predicted_SVM  = SVM.predict(X_test_cv)\n",
    "print(classification_report(Y_test, predicted_SVM))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f8cb2c7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['positive']\n",
      "['negative']\n"
     ]
    }
   ],
   "source": [
    "# TEST A CUSTOM STRING\n",
    "\n",
    "def predict_sentiment(custom_string):\n",
    "    custom_string = custom_string.replace('<.*?>', '')\n",
    "    custom_string_tokens = word_tokenize(custom_string)\n",
    "    custom_string_tokens = [item for item in custom_string_tokens if item not in stop_words]\n",
    "    \n",
    "    if normalize == 'stemming':\n",
    "        custom_string_tokens = [ps.stem(item) for item in custom_string_tokens]\n",
    "    if normalize == 'lemmatization':\n",
    "        custom_string_tokens = [lemmatizer.lemmatize(item) for item in custom_string_tokens]\n",
    "\n",
    "    custom_string_seq = [str(custom_string_tokens)]\n",
    "    custom_string_cv = cv.transform(custom_string_seq)\n",
    "\n",
    "    return SVM.predict(custom_string_cv)\n",
    "\n",
    "# Predicting a positive review\n",
    "test_positive = predict_sentiment(\"I loved this movie. It was so good. I would recommend it to everyone. I would watch it again and again. I loved the acting and the story. It was so good\")\n",
    "assert test_positive[0] == 'positive'\n",
    "\n",
    "# Predicting a negative review\n",
    "test_negative = predict_sentiment(\"I hated this movie. It was so bad. I would not recommend it to anyone. I would not watch it again and again. I hated the acting and the story. It was so bad\")\n",
    "assert test_negative[0] == 'negative'\n",
    "\n",
    "print(test_positive)\n",
    "print(test_negative)\n",
    "\n",
    "while True:\n",
    "    custom_string = input(\"Enter your review: \")\n",
    "    if custom_string == \"exit\" or custom_string == \"\":\n",
    "        break\n",
    "    \n",
    "    print(\"Input: \", custom_string)\n",
    "    print(\"Sentiment: \" + predict_sentiment(custom_string)[0] + \"\\n\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
